{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "42e3ba3d",
   "metadata": {},
   "source": [
    "! pip install azure-search-documents --pre\n",
    "! pip install openai\n",
    "! pip install python-dotenv\n",
    "! pip install --upgrade azure-search-documents\n",
    "!pip install --upgrade langchain\n",
    "! pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e521ab16",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch, \n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    ")  \n",
    "\n",
    "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import tempfile\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from azure.core.exceptions import HttpResponseError \n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "7316091c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment variables  \n",
    "#load_dotenv()  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
    "credential = AzureKeyCredential(\"jA50VnyT0OAeuHpY0RxJGz7z9kfrmdj7YF5PHYiPXjAzSeBS6hJg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "efe8f7fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://testingchat.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"e8143eabb02541259054426630db12a7\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "model: str = \"text-embedding-ada-002\"\n",
    "#pdf_file_name = \"Alaska Airlines and Horizon Air SMS Manual.pdf\"     \n",
    "\n",
    "AZURE_SEARCH_SERVICE_ENDPOINT = \"https://genaisafety.search.windows.net\"\n",
    "AZURE_SEARCH_ADMIN_KEY = \"jA50VnyT0OAeuHpY0RxJGz7z9kfrmdj7YF5PHYiPXjAzSeBS6hJg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d84a7b58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(AZURE_SEARCH_INDEX_NAME, fields):\n",
    "    vector_search = VectorSearch(\n",
    "        algorithm_configurations=[\n",
    "            HnswVectorSearchAlgorithmConfiguration(\n",
    "                name=\"my-vector-config\",\n",
    "                kind=\"hnsw\",\n",
    "                parameters={\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"my-semantic-config\",\n",
    "        prioritized_fields=PrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"id\"),\n",
    "            prioritized_keywords_fields=[SemanticField(field_name=\"content\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index with the semantic settings\n",
    "    index = SearchIndex(name=AZURE_SEARCH_INDEX_NAME, fields=fields,\n",
    "                        vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "\n",
    "    result = search_client.create_or_update_index(index)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d6a9746",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = OpenAIEmbeddings(deployment = model,chunk_size=1) \n",
    "\n",
    "# Replace with your actual connection string\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=storageaccountgenai;AccountKey=WAfyL4uqWgarFdx1ibNgWv9lmOINODLuN6nnLSQLgE/iuHhKGi1pYd6NQJ6LBnZO/DnnQfhbNSWi+AStsOLf1Q==;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# Replace with your container name\n",
    "container_name = \"safety\"\n",
    "\n",
    "# Create a BlobServiceClient using the connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get a reference to the container\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f224587e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vw-etq-events-fast\n",
      "vw-reportit-allreport-fast\n"
     ]
    }
   ],
   "source": [
    "blobs = container_client.list_blobs()\n",
    "dict_data = []\n",
    "for blob in blobs:\n",
    "    if blob.name.startswith(\"etq-data\"):\n",
    "        #blob_client = container_client.get_blob_client(blob)\n",
    "        pdf_file_name=str(blob.name).split(\"/\")[1] \n",
    "        bid=str(blob.name).split(\"/\")[1].split(\".\")[0].lower().replace(\" \",\"_\").replace(\"_\",\"-\")\n",
    "        print(bid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "7e14c676",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etq-data/vw-ETQ-Events-fast\n",
      "etq-data/vw_ReportIt_AllReport_fast\n",
      "dict_keys(['vw-ETQ-Events-fast', 'vw_ReportIt_AllReport_fast'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Replace with your actual connection string\n",
    "azure_connection_string = \"DefaultEndpointsProtocol=https;AccountName=storageaccountgenai;AccountKey=WAfyL4uqWgarFdx1ibNgWv9lmOINODLuN6nnLSQLgE/iuHhKGi1pYd6NQJ6LBnZO/DnnQfhbNSWi+AStsOLf1Q==;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# Replace with your container name\n",
    "container_name = \"safety\"\n",
    "\n",
    "try:\n",
    "    # Connect to Azure Blob Storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(azure_connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # List all blobs in the container\n",
    "    blobs = container_client.list_blobs()\n",
    "\n",
    "    # Create an empty list to store DataFrame objects\n",
    "    dfs = {}\n",
    "\n",
    "    # Loop through the blobs and process those that match the prefix\n",
    "    for blob in blobs:\n",
    "        if blob.name.startswith(\"etq-data\"):\n",
    "            # Extract file name and bid name\n",
    "            file_name = str(blob.name).split(\"/\")[1]\n",
    "            bid_name = str(blob.name).split(\"/\")[1].split(\".\")[0].lower().replace(\" \", \"_\").replace(\"_\", \"-\")\n",
    "\n",
    "            # Get the blob client for reading the blob data\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            print(blob.name)\n",
    "\n",
    "            # Download the blob data as a stream\n",
    "            blob_data = blob_client.download_blob()\n",
    "\n",
    "            # Read the stream as a string and then into a Pandas DataFrame\n",
    "            blob_text = blob_data.readall().decode('utf-8')\n",
    "            df = pd.read_csv(io.StringIO(blob_text))\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            dfs[file_name] = df\n",
    "\n",
    "    # Now you can work with the list of DataFrames (dfs) as needed\n",
    "    print(dfs.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "625c9052",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Division</th>\n",
       "      <th>INCIDENT_ID</th>\n",
       "      <th>EtqNumber</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Flight</th>\n",
       "      <th>Tail</th>\n",
       "      <th>Risk</th>\n",
       "      <th>ReportType</th>\n",
       "      <th>IrropType</th>\n",
       "      <th>...</th>\n",
       "      <th>RiskNotes</th>\n",
       "      <th>CreatedDate</th>\n",
       "      <th>Likelihood</th>\n",
       "      <th>Severity</th>\n",
       "      <th>ErrStation</th>\n",
       "      <th>DiscStation</th>\n",
       "      <th>EventLevel</th>\n",
       "      <th>DAGSRB</th>\n",
       "      <th>InvestReq</th>\n",
       "      <th>Time</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inflight</td>\n",
       "      <td>205</td>\n",
       "      <td>204</td>\n",
       "      <td>2016-10-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2094</td>\n",
       "      <td>428QX</td>\n",
       "      <td>1</td>\n",
       "      <td>Inflight : Other Event</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>A</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Division  INCIDENT_ID  EtqNumber   EventDate Airline Flight   Tail Risk  \\\n",
       "0  Inflight          205        204  2016-10-28     NaN   2094  428QX    1   \n",
       "\n",
       "               ReportType IrropType  ... RiskNotes CreatedDate Likelihood  \\\n",
       "0  Inflight : Other Event       NaN  ...       NaN         NaN        NaN   \n",
       "\n",
       "  Severity ErrStation DiscStation EventLevel DAGSRB  InvestReq Time  \n",
       "0      NaN        NaN         NaN          A    NaN        NaN  NaN  \n",
       "\n",
       "[1 rows x 30 columns]"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['vw-ETQ-Events-fast'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "097be880",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carrier</th>\n",
       "      <th>EventType</th>\n",
       "      <th>SubType</th>\n",
       "      <th>Division</th>\n",
       "      <th>ID</th>\n",
       "      <th>ObjectId</th>\n",
       "      <th>WeekStart</th>\n",
       "      <th>SubmittedDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Risk</th>\n",
       "      <th>...</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>Recommendations</th>\n",
       "      <th>Linked_eventNumber</th>\n",
       "      <th>Source</th>\n",
       "      <th>ErrorStation</th>\n",
       "      <th>DiscoveredStation</th>\n",
       "      <th>PSID</th>\n",
       "      <th>DISPLAY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AS</td>\n",
       "      <td>Report</td>\n",
       "      <td>Irreg</td>\n",
       "      <td>Inflight</td>\n",
       "      <td>49020</td>\n",
       "      <td>49051</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-01 00:29:25</td>\n",
       "      <td>Inflight : Catering/Service Interruption</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Was only catered with one beverage cart on a d...</td>\n",
       "      <td>Meal Issue</td>\n",
       "      <td>2017-12-31 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39303.0</td>\n",
       "      <td>Mobile Application</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1050875</td>\n",
       "      <td>Alexandra Prewett</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Carrier EventType SubType  Division     ID  ObjectId   WeekStart  \\\n",
       "0      AS    Report   Irreg  Inflight  49020     49051  2018-01-01   \n",
       "\n",
       "         SubmittedDate                                     Title  Risk  ...  \\\n",
       "0  2018-01-01 00:29:25  Inflight : Catering/Service Interruption   NaN  ...   \n",
       "\n",
       "                                         Description    Category  \\\n",
       "0  Was only catered with one beverage cart on a d...  Meal Issue   \n",
       "\n",
       "             EventDate Recommendations Linked_eventNumber              Source  \\\n",
       "0  2017-12-31 12:00:00             NaN            39303.0  Mobile Application   \n",
       "\n",
       "  ErrorStation  DiscoveredStation     PSID       DISPLAY_NAME  \n",
       "0          NaN                NaN  1050875  Alexandra Prewett  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['vw_ReportIt_AllReport_fast'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "0edfbbe3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Division_x</th>\n",
       "      <th>INCIDENT_ID</th>\n",
       "      <th>EtqNumber</th>\n",
       "      <th>EventDate_x</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Flight_x</th>\n",
       "      <th>Tail_x</th>\n",
       "      <th>Risk_x</th>\n",
       "      <th>ReportType</th>\n",
       "      <th>IrropType</th>\n",
       "      <th>...</th>\n",
       "      <th>Description_y</th>\n",
       "      <th>Category</th>\n",
       "      <th>EventDate_y</th>\n",
       "      <th>Recommendations</th>\n",
       "      <th>Linked_eventNumber</th>\n",
       "      <th>Source</th>\n",
       "      <th>ErrorStation</th>\n",
       "      <th>DiscoveredStation</th>\n",
       "      <th>PSID</th>\n",
       "      <th>DISPLAY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inflight</td>\n",
       "      <td>205</td>\n",
       "      <td>204</td>\n",
       "      <td>2016-10-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2094</td>\n",
       "      <td>428QX</td>\n",
       "      <td>1</td>\n",
       "      <td>Inflight : Other Event</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Division_x  INCIDENT_ID  EtqNumber EventDate_x Airline Flight_x Tail_x  \\\n",
       "0   Inflight          205        204  2016-10-28     NaN     2094  428QX   \n",
       "\n",
       "  Risk_x              ReportType IrropType  ... Description_y Category  \\\n",
       "0      1  Inflight : Other Event       NaN  ...           NaN      NaN   \n",
       "\n",
       "  EventDate_y Recommendations Linked_eventNumber Source ErrorStation  \\\n",
       "0         NaN             NaN                NaN    NaN          NaN   \n",
       "\n",
       "  DiscoveredStation  PSID DISPLAY_NAME  \n",
       "0               NaN   NaN          NaN  \n",
       "\n",
       "[1 rows x 53 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Perform a left join between etq_df and ri_df\n",
    "merged_df = pd.merge(dfs['vw-ETQ-Events-fast'], dfs['vw_ReportIt_AllReport_fast'], left_on='EtqNumber', right_on='Linked_eventNumber', how='left')\n",
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "d34456c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Division_x', 'INCIDENT_ID', 'EtqNumber', 'EventDate_x', 'Airline',\n",
       "       'Flight_x', 'Tail_x', 'Risk_x', 'ReportType', 'IrropType', 'Cause',\n",
       "       'CauseOther', 'Subject', 'Description_x', 'Origin', 'Destination',\n",
       "       'Location_x', 'WSRDate', 'OPLEOI', 'AnalystNotes', 'RiskNotes',\n",
       "       'CreatedDate', 'Likelihood', 'Severity', 'ErrStation', 'DiscStation',\n",
       "       'EventLevel', 'DAGSRB', 'InvestReq', 'Time', 'Carrier', 'EventType',\n",
       "       'SubType', 'Division_y', 'ID', 'ObjectId', 'WeekStart', 'SubmittedDate',\n",
       "       'Title', 'Risk_y', 'Location_y', 'Tail_y', 'Flight_y', 'Description_y',\n",
       "       'Category', 'EventDate_y', 'Recommendations', 'Linked_eventNumber',\n",
       "       'Source', 'ErrorStation', 'DiscoveredStation', 'PSID', 'DISPLAY_NAME'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "d02d577c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Division_x</th>\n",
       "      <th>EventDate_x</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description_x</th>\n",
       "      <th>Recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191392</th>\n",
       "      <td>407297.0</td>\n",
       "      <td>Inflight</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>Pax travelling with PETC; not compliant with p...</td>\n",
       "      <td>- Report # 407297\\n \\nWendy Boyer in 20C had h...</td>\n",
       "      <td>Give her a warning.  If she does it again she ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID Division_x EventDate_x  \\\n",
       "191392  407297.0   Inflight  2023-01-15   \n",
       "\n",
       "                                                  Subject  \\\n",
       "191392  Pax travelling with PETC; not compliant with p...   \n",
       "\n",
       "                                            Description_x  \\\n",
       "191392  - Report # 407297\\n \\nWendy Boyer in 20C had h...   \n",
       "\n",
       "                                          Recommendations  \n",
       "191392  Give her a warning.  If she does it again she ...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Apply the filtering conditions\n",
    "event_start_date = '2023-01-01'\n",
    "event_end_date = '2023-02-01'\n",
    "\n",
    "filtered_df = merged_df[(merged_df['EventDate_x'] >= event_start_date) &\n",
    "                        (merged_df['EventDate_x'] < event_end_date) &\n",
    "                        (merged_df['Recommendations'].notnull())]\n",
    "\n",
    "# 3. Select the desired columns\n",
    "selected_columns = ['ID','Division_x', 'EventDate_x', 'Subject', 'Description_x', 'Recommendations']\n",
    "result_df = filtered_df[selected_columns]\n",
    "result_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "bab4d6b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the total number of rows in the DataFrame\n",
    "total_rows = len(result_df)\n",
    "\n",
    "# Calculate the number of rows in each half\n",
    "rows_per_half = total_rows // 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "0a63cb8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slices of the DataFrame for each half\n",
    "dataframes_list = []\n",
    "data_dict_list = []\n",
    "partitionid = []\n",
    "ID = []\n",
    "Division = []\n",
    "EventDate = []\n",
    "Subject = []\n",
    "Description = []\n",
    "Recommendations = []\n",
    "for i in range(200):\n",
    "    start_row = i * rows_per_half\n",
    "    end_row = (i + 1) * rows_per_half\n",
    "    df_slice = result_df.iloc[start_row:end_row]\n",
    "    dataframes_list.append(df_slice)\n",
    "    #convert DF to JSON\n",
    "    json_data = df_slice.to_json()\n",
    "    data_dict = json.loads(json_data)\n",
    "    data_dict_list.append(data_dict)\n",
    "    #partitionID\n",
    "    partitionid.append(i)\n",
    "    #other fields\n",
    "    ID.append(json.dumps(data_dict['ID']))\n",
    "    Division.append(json.dumps(data_dict['Division_x']))\n",
    "    EventDate.append(json.dumps(data_dict['EventDate_x']))\n",
    "    Subject.append(json.dumps(data_dict['Subject']))\n",
    "    Description.append(json.dumps(data_dict['Description_x']))\n",
    "    Recommendations.append(json.dumps(data_dict['Recommendations']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "d618e2b0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitionid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "cec4f7d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vw-reportit-allreport-fast-index\n"
     ]
    }
   ],
   "source": [
    "AZURE_SEARCH_INDEX_NAME=bid_name + \"-index\"\n",
    "print(AZURE_SEARCH_INDEX_NAME)\n",
    "\n",
    "search_client = SearchIndexClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, credential=credential)\n",
    "search_client_v2 = SearchClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, index_name=AZURE_SEARCH_INDEX_NAME, credential=credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "2297b138",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vw-reportit-allreport-fast-index'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AZURE_SEARCH_INDEX_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "beea342f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vw-reportit-allreport-fast-index created index\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "Vector store added successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "except Exception as e:\n",
    "    existing_index = None\n",
    "    # The index doesn't exist, so create a new one\n",
    "    \n",
    "metadata = result_df.columns.tolist()\n",
    "if existing_index == None:\n",
    "    fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"Division\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"EventDate\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"subject\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"description\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"Recommendations\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"metadata\", type=SearchFieldDataType.String, searchable=True),\n",
    "    ]\n",
    "\n",
    "    result = create_index(AZURE_SEARCH_INDEX_NAME, fields)\n",
    "    print(f' {result.name} created index')                        \n",
    "    final_doc=[]\n",
    "    for index in range(len(partitionid)):\n",
    "        final_doc.append({'id': str(partitionid[index]), \n",
    "                          'Division': Division[index],\n",
    "                          'EventDate': EventDate[index],  #df.iloc[index],\n",
    "                          'subject': Subject[index],\n",
    "                          'description': Description[index],\n",
    "                          'Recommendations': Recommendations[index],\n",
    "                          'content': str(data_dict_list[index]),\n",
    "                          'metadata': str(metadata),\n",
    "                         })\n",
    "\n",
    "     \n",
    "    # Define a list to store batches of documents\n",
    "    #document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "    # Upload documents in batches\n",
    "    for batch in final_doc:\n",
    "        try:\n",
    "            result = search_client_v2.upload_documents(batch)\n",
    "            print('documents uploaded')\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading documents: {e}\")\n",
    "            print(repr(e))\n",
    "            break\n",
    "\n",
    "    print(\"Vector store added successfully\")\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        # Check if the index exists\n",
    "        # existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "        # print(f\"Index '{AZURE_SEARCH_INDEX_NAME}' already exists. You can use the existing index.\")\n",
    "        final_doc=[]\n",
    "        for index in range(len(partitionid)):\n",
    "            final_doc.append({'id': str(partitionid[index]), \n",
    "                              'Division': Division[index],\n",
    "                              'EventDate': EventDate[index],  #df.iloc[index],\n",
    "                              'subject': Subject[index],\n",
    "                              'description': Description[index],\n",
    "                              'Recommendations': Recommendations[index],\n",
    "                              'content': str(data_dict_list[index]),\n",
    "                              'metadata': str(metadata),\n",
    "                             })\n",
    "\n",
    "        \n",
    "\n",
    "        # Define a list to store batches of documents\n",
    "        document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "        # Upload documents in batches\n",
    "        for batch in document_batches:\n",
    "            try:\n",
    "                result = search_client_v2.upload_documents(batch)\n",
    "                print('documents uploaded')\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading documents: {e}\") \n",
    "                print(repr(e))\n",
    "                break\n",
    "\n",
    "        print(\"Vector store added successfully\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcc31a18",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f4da8d58",
   "metadata": {},
   "source": [
    "New work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "id": "986f2d2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vw-reportit-allreport-fast-wc-index\n"
     ]
    }
   ],
   "source": [
    "AZURE_SEARCH_INDEX_NAME=bid_name + \"-wc-index\"\n",
    "print(AZURE_SEARCH_INDEX_NAME)\n",
    "\n",
    "\n",
    "search_client = SearchIndexClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, credential=credential)\n",
    "search_client_v2 = SearchClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, index_name=AZURE_SEARCH_INDEX_NAME, credential=credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "667d72d9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vw-reportit-allreport-fast-wc-index created index\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "Vector store added successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "except Exception as e:\n",
    "    existing_index = None\n",
    "    # The index doesn't exist, so create a new one\n",
    "    \n",
    "metadata = result_df.columns.tolist()\n",
    "if existing_index == None:\n",
    "    fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"metadata\", type=SearchFieldDataType.String, searchable=True),\n",
    "    ]\n",
    "\n",
    "    result = create_index(AZURE_SEARCH_INDEX_NAME, fields)\n",
    "    print(f' {result.name} created index')                        \n",
    "    final_doc=[]\n",
    "    for index in range(len(partitionid)):\n",
    "        final_doc.append({'id': str(partitionid[index]), \n",
    "                          'content': str(data_dict_list[index]),\n",
    "                          'metadata': str(metadata),\n",
    "                         })\n",
    "\n",
    "     \n",
    "    # Define a list to store batches of documents\n",
    "    #document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "    # Upload documents in batches\n",
    "    for batch in final_doc:\n",
    "        try:\n",
    "            result = search_client_v2.upload_documents(batch)\n",
    "            print('documents uploaded')\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading documents: {e}\")\n",
    "            print(repr(e))\n",
    "            break\n",
    "\n",
    "    print(\"Vector store added successfully\")\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        # Check if the index exists\n",
    "        # existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "        # print(f\"Index '{AZURE_SEARCH_INDEX_NAME}' already exists. You can use the existing index.\")\n",
    "        final_doc=[]\n",
    "        for index in range(len(partitionid)):\n",
    "            final_doc.append({'id': str(partitionid[index]), \n",
    "                              'content': str(data_dict_list[index]),\n",
    "                              'metadata': str(metadata),\n",
    "                             })\n",
    "\n",
    "        \n",
    "\n",
    "        # Define a list to store batches of documents\n",
    "        document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "        # Upload documents in batches\n",
    "        for batch in document_batches:\n",
    "            try:\n",
    "                result = search_client_v2.upload_documents(batch)\n",
    "                print('documents uploaded')\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading documents: {e}\") \n",
    "                print(repr(e))\n",
    "                break\n",
    "\n",
    "        print(\"Vector store added successfully\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57d5482c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
