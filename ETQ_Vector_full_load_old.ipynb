{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "824ceaa0",
   "metadata": {},
   "source": [
    "! pip install azure-search-documents --pre\n",
    "! pip install openai\n",
    "! pip install python-dotenv\n",
    "! pip install --upgrade azure-search-documents\n",
    "!pip install --upgrade langchain\n",
    "! pip install pandasql"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d987087b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch, \n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    ")  \n",
    "\n",
    "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import tempfile\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from azure.core.exceptions import HttpResponseError \n",
    "from langchain.embeddings.openai import OpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c94b352a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configure environment variables  \n",
    "#load_dotenv()  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
    "credential = AzureKeyCredential(\"jA50VnyT0OAeuHpY0RxJGz7z9kfrmdj7YF5PHYiPXjAzSeBS6hJg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "17c05150",
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://testingchat.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"e8143eabb02541259054426630db12a7\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "model: str = \"text-embedding-ada-002\"\n",
    "#pdf_file_name = \"Alaska Airlines and Horizon Air SMS Manual.pdf\"     \n",
    "\n",
    "AZURE_SEARCH_SERVICE_ENDPOINT = \"https://genaisafety.search.windows.net\"\n",
    "AZURE_SEARCH_ADMIN_KEY = \"jA50VnyT0OAeuHpY0RxJGz7z9kfrmdj7YF5PHYiPXjAzSeBS6hJg\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "ab2e93fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_index(AZURE_SEARCH_INDEX_NAME, fields):\n",
    "    vector_search = VectorSearch(\n",
    "        algorithm_configurations=[\n",
    "            HnswVectorSearchAlgorithmConfiguration(\n",
    "                name=\"my-vector-config\",\n",
    "                kind=\"hnsw\",\n",
    "                parameters={\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"my-semantic-config\",\n",
    "        prioritized_fields=PrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"id\"),\n",
    "            prioritized_keywords_fields=[SemanticField(field_name=\"content\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index with the semantic settings\n",
    "    index = SearchIndex(name=AZURE_SEARCH_INDEX_NAME, fields=fields,\n",
    "                        vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "\n",
    "    result = search_client.create_or_update_index(index)\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a6f8dd6d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#embeddings = OpenAIEmbeddings(deployment = model,chunk_size=1) \n",
    "\n",
    "# Replace with your actual connection string\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=storageaccountgenai;AccountKey=WAfyL4uqWgarFdx1ibNgWv9lmOINODLuN6nnLSQLgE/iuHhKGi1pYd6NQJ6LBnZO/DnnQfhbNSWi+AStsOLf1Q==;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# Replace with your container name\n",
    "container_name = \"safety\"\n",
    "\n",
    "# Create a BlobServiceClient using the connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get a reference to the container\n",
    "container_client = blob_service_client.get_container_client(container_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "14a4387d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vw-etq-events-fast\n",
      "vw-reportit-allreport-fast\n"
     ]
    }
   ],
   "source": [
    "blobs = container_client.list_blobs()\n",
    "dict_data = []\n",
    "for blob in blobs:\n",
    "    if blob.name.startswith(\"etq-data\"):\n",
    "        #blob_client = container_client.get_blob_client(blob)\n",
    "        pdf_file_name=str(blob.name).split(\"/\")[1] \n",
    "        bid=str(blob.name).split(\"/\")[1].split(\".\")[0].lower().replace(\" \",\"_\").replace(\"_\",\"-\")\n",
    "        print(bid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "8dd35854",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "etq-data/vw-ETQ-Events-fast\n",
      "etq-data/vw_ReportIt_AllReport_fast\n",
      "dict_keys(['vw-ETQ-Events-fast', 'vw_ReportIt_AllReport_fast'])\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import io\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "\n",
    "# Replace with your actual connection string\n",
    "azure_connection_string = \"DefaultEndpointsProtocol=https;AccountName=storageaccountgenai;AccountKey=WAfyL4uqWgarFdx1ibNgWv9lmOINODLuN6nnLSQLgE/iuHhKGi1pYd6NQJ6LBnZO/DnnQfhbNSWi+AStsOLf1Q==;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# Replace with your container name\n",
    "container_name = \"safety\"\n",
    "\n",
    "try:\n",
    "    # Connect to Azure Blob Storage\n",
    "    blob_service_client = BlobServiceClient.from_connection_string(azure_connection_string)\n",
    "    container_client = blob_service_client.get_container_client(container_name)\n",
    "\n",
    "    # List all blobs in the container\n",
    "    blobs = container_client.list_blobs()\n",
    "\n",
    "    # Create an empty list to store DataFrame objects\n",
    "    dfs = {}\n",
    "\n",
    "    # Loop through the blobs and process those that match the prefix\n",
    "    for blob in blobs:\n",
    "        if blob.name.startswith(\"etq-data\"):\n",
    "            # Extract file name and bid name\n",
    "            file_name = str(blob.name).split(\"/\")[1]\n",
    "            bid_name = str(blob.name).split(\"/\")[1].split(\".\")[0].lower().replace(\" \", \"_\").replace(\"_\", \"-\")\n",
    "\n",
    "            # Get the blob client for reading the blob data\n",
    "            blob_client = container_client.get_blob_client(blob.name)\n",
    "            print(blob.name)\n",
    "\n",
    "            # Download the blob data as a stream\n",
    "            blob_data = blob_client.download_blob()\n",
    "\n",
    "            # Read the stream as a string and then into a Pandas DataFrame\n",
    "            blob_text = blob_data.readall().decode('utf-8')\n",
    "            df = pd.read_csv(io.StringIO(blob_text))\n",
    "\n",
    "            # Append the DataFrame to the list\n",
    "            dfs[file_name] = df\n",
    "\n",
    "    # Now you can work with the list of DataFrames (dfs) as needed\n",
    "    print(dfs.keys())\n",
    "\n",
    "except Exception as e:\n",
    "    print(f'Error: {str(e)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "46119543",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'dfs' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m dfs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mvw-ETQ-Events-fast\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mhead(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'dfs' is not defined"
     ]
    }
   ],
   "source": [
    "dfs['vw-ETQ-Events-fast'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "370273e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Carrier</th>\n",
       "      <th>EventType</th>\n",
       "      <th>SubType</th>\n",
       "      <th>Division</th>\n",
       "      <th>ID</th>\n",
       "      <th>ObjectId</th>\n",
       "      <th>WeekStart</th>\n",
       "      <th>SubmittedDate</th>\n",
       "      <th>Title</th>\n",
       "      <th>Risk</th>\n",
       "      <th>...</th>\n",
       "      <th>Description</th>\n",
       "      <th>Category</th>\n",
       "      <th>EventDate</th>\n",
       "      <th>Recommendations</th>\n",
       "      <th>Linked_eventNumber</th>\n",
       "      <th>Source</th>\n",
       "      <th>ErrorStation</th>\n",
       "      <th>DiscoveredStation</th>\n",
       "      <th>PSID</th>\n",
       "      <th>DISPLAY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AS</td>\n",
       "      <td>Report</td>\n",
       "      <td>Irreg</td>\n",
       "      <td>Inflight</td>\n",
       "      <td>49020</td>\n",
       "      <td>49051</td>\n",
       "      <td>2018-01-01</td>\n",
       "      <td>2018-01-01 00:29:25</td>\n",
       "      <td>Inflight : Catering/Service Interruption</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>Was only catered with one beverage cart on a d...</td>\n",
       "      <td>Meal Issue</td>\n",
       "      <td>2017-12-31 12:00:00</td>\n",
       "      <td>NaN</td>\n",
       "      <td>39303.0</td>\n",
       "      <td>Mobile Application</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1050875</td>\n",
       "      <td>Alexandra Prewett</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 23 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Carrier EventType SubType  Division     ID  ObjectId   WeekStart  \\\n",
       "0      AS    Report   Irreg  Inflight  49020     49051  2018-01-01   \n",
       "\n",
       "         SubmittedDate                                     Title  Risk  ...  \\\n",
       "0  2018-01-01 00:29:25  Inflight : Catering/Service Interruption   NaN  ...   \n",
       "\n",
       "                                         Description    Category  \\\n",
       "0  Was only catered with one beverage cart on a d...  Meal Issue   \n",
       "\n",
       "             EventDate Recommendations Linked_eventNumber              Source  \\\n",
       "0  2017-12-31 12:00:00             NaN            39303.0  Mobile Application   \n",
       "\n",
       "  ErrorStation  DiscoveredStation     PSID       DISPLAY_NAME  \n",
       "0          NaN                NaN  1050875  Alexandra Prewett  \n",
       "\n",
       "[1 rows x 23 columns]"
      ]
     },
     "execution_count": 79,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfs['vw_ReportIt_AllReport_fast'].head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "b229c5d1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Division_x</th>\n",
       "      <th>INCIDENT_ID</th>\n",
       "      <th>EtqNumber</th>\n",
       "      <th>EventDate_x</th>\n",
       "      <th>Airline</th>\n",
       "      <th>Flight_x</th>\n",
       "      <th>Tail_x</th>\n",
       "      <th>Risk_x</th>\n",
       "      <th>ReportType</th>\n",
       "      <th>IrropType</th>\n",
       "      <th>...</th>\n",
       "      <th>Description_y</th>\n",
       "      <th>Category</th>\n",
       "      <th>EventDate_y</th>\n",
       "      <th>Recommendations</th>\n",
       "      <th>Linked_eventNumber</th>\n",
       "      <th>Source</th>\n",
       "      <th>ErrorStation</th>\n",
       "      <th>DiscoveredStation</th>\n",
       "      <th>PSID</th>\n",
       "      <th>DISPLAY_NAME</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Inflight</td>\n",
       "      <td>205</td>\n",
       "      <td>204</td>\n",
       "      <td>2016-10-28</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2094</td>\n",
       "      <td>428QX</td>\n",
       "      <td>1</td>\n",
       "      <td>Inflight : Other Event</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "  Division_x  INCIDENT_ID  EtqNumber EventDate_x Airline Flight_x Tail_x  \\\n",
       "0   Inflight          205        204  2016-10-28     NaN     2094  428QX   \n",
       "\n",
       "  Risk_x              ReportType IrropType  ... Description_y Category  \\\n",
       "0      1  Inflight : Other Event       NaN  ...           NaN      NaN   \n",
       "\n",
       "  EventDate_y Recommendations Linked_eventNumber Source ErrorStation  \\\n",
       "0         NaN             NaN                NaN    NaN          NaN   \n",
       "\n",
       "  DiscoveredStation  PSID DISPLAY_NAME  \n",
       "0               NaN   NaN          NaN  \n",
       "\n",
       "[1 rows x 53 columns]"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Perform a left join between etq_df and ri_df\n",
    "merged_df = pd.merge(dfs['vw-ETQ-Events-fast'], dfs['vw_ReportIt_AllReport_fast'], left_on='EtqNumber', right_on='Linked_eventNumber', how='left')\n",
    "merged_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "518ec433",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Division_x', 'INCIDENT_ID', 'EtqNumber', 'EventDate_x', 'Airline',\n",
       "       'Flight_x', 'Tail_x', 'Risk_x', 'ReportType', 'IrropType', 'Cause',\n",
       "       'CauseOther', 'Subject', 'Description_x', 'Origin', 'Destination',\n",
       "       'Location_x', 'WSRDate', 'OPLEOI', 'AnalystNotes', 'RiskNotes',\n",
       "       'CreatedDate', 'Likelihood', 'Severity', 'ErrStation', 'DiscStation',\n",
       "       'EventLevel', 'DAGSRB', 'InvestReq', 'Time', 'Carrier', 'EventType',\n",
       "       'SubType', 'Division_y', 'ID', 'ObjectId', 'WeekStart', 'SubmittedDate',\n",
       "       'Title', 'Risk_y', 'Location_y', 'Tail_y', 'Flight_y', 'Description_y',\n",
       "       'Category', 'EventDate_y', 'Recommendations', 'Linked_eventNumber',\n",
       "       'Source', 'ErrorStation', 'DiscoveredStation', 'PSID', 'DISPLAY_NAME'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_df.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "9bbb9ebe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>Division_x</th>\n",
       "      <th>EventDate_x</th>\n",
       "      <th>Subject</th>\n",
       "      <th>Description_x</th>\n",
       "      <th>Recommendations</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>191392</th>\n",
       "      <td>407297.0</td>\n",
       "      <td>Inflight</td>\n",
       "      <td>2023-01-15</td>\n",
       "      <td>Pax travelling with PETC; not compliant with p...</td>\n",
       "      <td>- Report # 407297\\n \\nWendy Boyer in 20C had h...</td>\n",
       "      <td>Give her a warning.  If she does it again she ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              ID Division_x EventDate_x  \\\n",
       "191392  407297.0   Inflight  2023-01-15   \n",
       "\n",
       "                                                  Subject  \\\n",
       "191392  Pax travelling with PETC; not compliant with p...   \n",
       "\n",
       "                                            Description_x  \\\n",
       "191392  - Report # 407297\\n \\nWendy Boyer in 20C had h...   \n",
       "\n",
       "                                          Recommendations  \n",
       "191392  Give her a warning.  If she does it again she ...  "
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Apply the filtering conditions\n",
    "event_start_date = '2023-01-01'\n",
    "event_end_date = '2023-02-01'\n",
    "\n",
    "filtered_df = merged_df[(merged_df['EventDate_x'] >= event_start_date) &\n",
    "                        (merged_df['EventDate_x'] < event_end_date) &\n",
    "                        (merged_df['Recommendations'].notnull())]\n",
    "\n",
    "# 3. Select the desired columns\n",
    "selected_columns = ['ID','Division_x', 'EventDate_x', 'Subject', 'Description_x', 'Recommendations']\n",
    "result_df = filtered_df[selected_columns]\n",
    "result_df.head(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "0182e652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Determine the total number of rows in the DataFrame\n",
    "total_rows = len(result_df)\n",
    "\n",
    "# Calculate the number of rows in each half\n",
    "rows_per_half = total_rows // 200"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "id": "54136819",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create slices of the DataFrame for each half\n",
    "dataframes_list = []\n",
    "data_dict_list = []\n",
    "partitionid = []\n",
    "ID = []\n",
    "Division = []\n",
    "EventDate = []\n",
    "Subject = []\n",
    "Description = []\n",
    "Recommendations = []\n",
    "for i in range(200):\n",
    "    start_row = i * rows_per_half\n",
    "    end_row = (i + 1) * rows_per_half\n",
    "    df_slice = result_df.iloc[start_row:end_row]\n",
    "    dataframes_list.append(df_slice)\n",
    "    #convert DF to JSON\n",
    "    json_data = df_slice.to_json()\n",
    "    data_dict = json.loads(json_data)\n",
    "    data_dict_list.append(data_dict)\n",
    "    #partitionID\n",
    "    partitionid.append(i)\n",
    "    #other fields\n",
    "    ID.append(json.dumps(data_dict['ID']))\n",
    "    Division.append(json.dumps(data_dict['Division_x']))\n",
    "    EventDate.append(json.dumps(data_dict['EventDate_x']))\n",
    "    Subject.append(json.dumps(data_dict['Subject']))\n",
    "    Description.append(json.dumps(data_dict['Description_x']))\n",
    "    Recommendations.append(json.dumps(data_dict['Recommendations']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "id": "633014d4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0,\n",
       " 1,\n",
       " 2,\n",
       " 3,\n",
       " 4,\n",
       " 5,\n",
       " 6,\n",
       " 7,\n",
       " 8,\n",
       " 9,\n",
       " 10,\n",
       " 11,\n",
       " 12,\n",
       " 13,\n",
       " 14,\n",
       " 15,\n",
       " 16,\n",
       " 17,\n",
       " 18,\n",
       " 19,\n",
       " 20,\n",
       " 21,\n",
       " 22,\n",
       " 23,\n",
       " 24,\n",
       " 25,\n",
       " 26,\n",
       " 27,\n",
       " 28,\n",
       " 29,\n",
       " 30,\n",
       " 31,\n",
       " 32,\n",
       " 33,\n",
       " 34,\n",
       " 35,\n",
       " 36,\n",
       " 37,\n",
       " 38,\n",
       " 39,\n",
       " 40,\n",
       " 41,\n",
       " 42,\n",
       " 43,\n",
       " 44,\n",
       " 45,\n",
       " 46,\n",
       " 47,\n",
       " 48,\n",
       " 49,\n",
       " 50,\n",
       " 51,\n",
       " 52,\n",
       " 53,\n",
       " 54,\n",
       " 55,\n",
       " 56,\n",
       " 57,\n",
       " 58,\n",
       " 59,\n",
       " 60,\n",
       " 61,\n",
       " 62,\n",
       " 63,\n",
       " 64,\n",
       " 65,\n",
       " 66,\n",
       " 67,\n",
       " 68,\n",
       " 69,\n",
       " 70,\n",
       " 71,\n",
       " 72,\n",
       " 73,\n",
       " 74,\n",
       " 75,\n",
       " 76,\n",
       " 77,\n",
       " 78,\n",
       " 79,\n",
       " 80,\n",
       " 81,\n",
       " 82,\n",
       " 83,\n",
       " 84,\n",
       " 85,\n",
       " 86,\n",
       " 87,\n",
       " 88,\n",
       " 89,\n",
       " 90,\n",
       " 91,\n",
       " 92,\n",
       " 93,\n",
       " 94,\n",
       " 95,\n",
       " 96,\n",
       " 97,\n",
       " 98,\n",
       " 99,\n",
       " 100,\n",
       " 101,\n",
       " 102,\n",
       " 103,\n",
       " 104,\n",
       " 105,\n",
       " 106,\n",
       " 107,\n",
       " 108,\n",
       " 109,\n",
       " 110,\n",
       " 111,\n",
       " 112,\n",
       " 113,\n",
       " 114,\n",
       " 115,\n",
       " 116,\n",
       " 117,\n",
       " 118,\n",
       " 119,\n",
       " 120,\n",
       " 121,\n",
       " 122,\n",
       " 123,\n",
       " 124,\n",
       " 125,\n",
       " 126,\n",
       " 127,\n",
       " 128,\n",
       " 129,\n",
       " 130,\n",
       " 131,\n",
       " 132,\n",
       " 133,\n",
       " 134,\n",
       " 135,\n",
       " 136,\n",
       " 137,\n",
       " 138,\n",
       " 139,\n",
       " 140,\n",
       " 141,\n",
       " 142,\n",
       " 143,\n",
       " 144,\n",
       " 145,\n",
       " 146,\n",
       " 147,\n",
       " 148,\n",
       " 149,\n",
       " 150,\n",
       " 151,\n",
       " 152,\n",
       " 153,\n",
       " 154,\n",
       " 155,\n",
       " 156,\n",
       " 157,\n",
       " 158,\n",
       " 159,\n",
       " 160,\n",
       " 161,\n",
       " 162,\n",
       " 163,\n",
       " 164,\n",
       " 165,\n",
       " 166,\n",
       " 167,\n",
       " 168,\n",
       " 169,\n",
       " 170,\n",
       " 171,\n",
       " 172,\n",
       " 173,\n",
       " 174,\n",
       " 175,\n",
       " 176,\n",
       " 177,\n",
       " 178,\n",
       " 179,\n",
       " 180,\n",
       " 181,\n",
       " 182,\n",
       " 183,\n",
       " 184,\n",
       " 185,\n",
       " 186,\n",
       " 187,\n",
       " 188,\n",
       " 189,\n",
       " 190,\n",
       " 191,\n",
       " 192,\n",
       " 193,\n",
       " 194,\n",
       " 195,\n",
       " 196,\n",
       " 197,\n",
       " 198,\n",
       " 199]"
      ]
     },
     "execution_count": 176,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "partitionid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "id": "525addb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vw-reportit-allreport-fast-index\n"
     ]
    }
   ],
   "source": [
    "AZURE_SEARCH_INDEX_NAME=bid_name + \"-index\"\n",
    "print(AZURE_SEARCH_INDEX_NAME)\n",
    "\n",
    "search_client = SearchIndexClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, credential=credential)\n",
    "search_client_v2 = SearchClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, index_name=AZURE_SEARCH_INDEX_NAME, credential=credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "id": "fb6eac0f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'vw-reportit-allreport-fast-index'"
      ]
     },
     "execution_count": 204,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "AZURE_SEARCH_INDEX_NAME "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "id": "043df4df",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vw-reportit-allreport-fast-index created index\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "Vector store added successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "except Exception as e:\n",
    "    existing_index = None\n",
    "    # The index doesn't exist, so create a new one\n",
    "    \n",
    "metadata = result_df.columns.tolist()\n",
    "if existing_index == None:\n",
    "    fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"Division\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"EventDate\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"subject\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"description\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"Recommendations\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"metadata\", type=SearchFieldDataType.String, searchable=True),\n",
    "    ]\n",
    "\n",
    "    result = create_index(AZURE_SEARCH_INDEX_NAME, fields)\n",
    "    print(f' {result.name} created index')                        \n",
    "    final_doc=[]\n",
    "    for index in range(len(partitionid)):\n",
    "        final_doc.append({'id': str(partitionid[index]), \n",
    "                          'Division': Division[index],\n",
    "                          'EventDate': EventDate[index],  #df.iloc[index],\n",
    "                          'subject': Subject[index],\n",
    "                          'description': Description[index],\n",
    "                          'Recommendations': Recommendations[index],\n",
    "                          'content': str(data_dict_list[index]),\n",
    "                          'metadata': str(metadata),\n",
    "                         })\n",
    "\n",
    "     \n",
    "    # Define a list to store batches of documents\n",
    "    #document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "    # Upload documents in batches\n",
    "    for batch in final_doc:\n",
    "        try:\n",
    "            result = search_client_v2.upload_documents(batch)\n",
    "            print('documents uploaded')\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading documents: {e}\")\n",
    "            print(repr(e))\n",
    "            break\n",
    "\n",
    "    print(\"Vector store added successfully\")\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        # Check if the index exists\n",
    "        # existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "        # print(f\"Index '{AZURE_SEARCH_INDEX_NAME}' already exists. You can use the existing index.\")\n",
    "        final_doc=[]\n",
    "        for index in range(len(partitionid)):\n",
    "            final_doc.append({'id': str(partitionid[index]), \n",
    "                              'Division': Division[index],\n",
    "                              'EventDate': EventDate[index],  #df.iloc[index],\n",
    "                              'subject': Subject[index],\n",
    "                              'description': Description[index],\n",
    "                              'Recommendations': Recommendations[index],\n",
    "                              'content': str(data_dict_list[index]),\n",
    "                              'metadata': str(metadata),\n",
    "                             })\n",
    "\n",
    "        \n",
    "\n",
    "        # Define a list to store batches of documents\n",
    "        document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "        # Upload documents in batches\n",
    "        for batch in document_batches:\n",
    "            try:\n",
    "                result = search_client_v2.upload_documents(batch)\n",
    "                print('documents uploaded')\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading documents: {e}\") \n",
    "                print(repr(e))\n",
    "                break\n",
    "\n",
    "        print(\"Vector store added successfully\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b92b196",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7a036b8b",
   "metadata": {},
   "source": [
    "New work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7f422fd5",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'bid_name' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[9], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m AZURE_SEARCH_INDEX_NAME\u001b[38;5;241m=\u001b[39mbid_name \u001b[38;5;241m+\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m-wc-index\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(AZURE_SEARCH_INDEX_NAME)\n\u001b[1;32m      5\u001b[0m search_client \u001b[38;5;241m=\u001b[39m SearchIndexClient(endpoint\u001b[38;5;241m=\u001b[39mAZURE_SEARCH_SERVICE_ENDPOINT, credential\u001b[38;5;241m=\u001b[39mcredential)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'bid_name' is not defined"
     ]
    }
   ],
   "source": [
    "AZURE_SEARCH_INDEX_NAME=bid_name + \"-wc-index\"\n",
    "print(AZURE_SEARCH_INDEX_NAME)\n",
    "\n",
    "\n",
    "search_client = SearchIndexClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, credential=credential)\n",
    "search_client_v2 = SearchClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, index_name=AZURE_SEARCH_INDEX_NAME, credential=credential)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "id": "e9036d7b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " vw-reportit-allreport-fast-wc-index created index\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "documents uploaded\n",
      "Vector store added successfully\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "except Exception as e:\n",
    "    existing_index = None\n",
    "    # The index doesn't exist, so create a new one\n",
    "    \n",
    "metadata = result_df.columns.tolist()\n",
    "if existing_index == None:\n",
    "    fields = [\n",
    "    SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "    SearchableField(name=\"content\", type=SearchFieldDataType.String, searchable=True),\n",
    "    SearchableField(name=\"metadata\", type=SearchFieldDataType.String, searchable=True),\n",
    "    ]\n",
    "\n",
    "    result = create_index(AZURE_SEARCH_INDEX_NAME, fields)\n",
    "    print(f' {result.name} created index')                        \n",
    "    final_doc=[]\n",
    "    for index in range(len(partitionid)):\n",
    "        final_doc.append({'id': str(partitionid[index]), \n",
    "                          'content': str(data_dict_list[index]),\n",
    "                          'metadata': str(metadata),\n",
    "                         })\n",
    "\n",
    "     \n",
    "    # Define a list to store batches of documents\n",
    "    #document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "    # Upload documents in batches\n",
    "    for batch in final_doc:\n",
    "        try:\n",
    "            result = search_client_v2.upload_documents(batch)\n",
    "            print('documents uploaded')\n",
    "        except Exception as e:\n",
    "            print(f\"Error uploading documents: {e}\")\n",
    "            print(repr(e))\n",
    "            break\n",
    "\n",
    "    print(\"Vector store added successfully\")\n",
    "\n",
    "else:\n",
    "    try:\n",
    "        # Check if the index exists\n",
    "        # existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "        # print(f\"Index '{AZURE_SEARCH_INDEX_NAME}' already exists. You can use the existing index.\")\n",
    "        final_doc=[]\n",
    "        for index in range(len(partitionid)):\n",
    "            final_doc.append({'id': str(partitionid[index]), \n",
    "                              'content': str(data_dict_list[index]),\n",
    "                              'metadata': str(metadata),\n",
    "                             })\n",
    "\n",
    "        \n",
    "\n",
    "        # Define a list to store batches of documents\n",
    "        document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "        # Upload documents in batches\n",
    "        for batch in document_batches:\n",
    "            try:\n",
    "                result = search_client_v2.upload_documents(batch)\n",
    "                print('documents uploaded')\n",
    "            except Exception as e:\n",
    "                print(f\"Error uploading documents: {e}\") \n",
    "                print(repr(e))\n",
    "                break\n",
    "\n",
    "        print(\"Vector store added successfully\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3d472fc",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "8cda3777",
   "metadata": {},
   "source": [
    "Vector Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b18a6572",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "vw-reportit-allreport-fast-wc-index\n"
     ]
    }
   ],
   "source": [
    "AZURE_SEARCH_INDEX_NAME= \"vw-reportit-allreport-fast-wc-index\"\n",
    "print(AZURE_SEARCH_INDEX_NAME)\n",
    "\n",
    "\n",
    "search_client = SearchIndexClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, credential=credential)\n",
    "search_client_v2 = SearchClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, index_name=AZURE_SEARCH_INDEX_NAME, credential=credential)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "ead4d208",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a search query\n",
    "search_query = \"Are we also seeing similar trends with Towbars?\"\n",
    "\n",
    "# Perform a search\n",
    "results = search_client_v2.search(search_text=search_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6791d39c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "azure.search.documents._paging.SearchItemPaged"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e3efd21b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'content': '{\\'ID\\': {\\'292802\\': 406392.0, \\'292803\\': 406379.0, \\'292807\\': 406530.0, \\'292808\\': 406360.0, \\'292809\\': 406361.0, \\'292811\\': 406356.0, \\'292812\\': 406366.0, \\'292814\\': 406227.0, \\'292816\\': 406265.0, \\'292820\\': 406323.0, \\'292821\\': 406325.0}, \\'Division_x\\': {\\'292802\\': \\'FlightOps\\', \\'292803\\': \\'FlightOps\\', \\'292807\\': \\'FlightOps\\', \\'292808\\': \\'FlightOps\\', \\'292809\\': \\'FlightOps\\', \\'292811\\': \\'FlightOps\\', \\'292812\\': \\'FlightOps\\', \\'292814\\': \\'Inflight\\', \\'292816\\': \\'Inflight\\', \\'292820\\': \\'Inflight\\', \\'292821\\': \\'Inflight\\'}, \\'EventDate_x\\': {\\'292802\\': \\'2023-01-10\\', \\'292803\\': \\'2023-01-10\\', \\'292807\\': \\'2023-01-10\\', \\'292808\\': \\'2023-01-10\\', \\'292809\\': \\'2023-01-10\\', \\'292811\\': \\'2023-01-04\\', \\'292812\\': \\'2023-01-03\\', \\'292814\\': \\'2023-01-09\\', \\'292816\\': \\'2023-01-09\\', \\'292820\\': \\'2023-01-09\\', \\'292821\\': \\'2023-01-09\\'}, \\'Subject\\': {\\'292802\\': \\'LAS - Deviated from the 11k ft restriction at KRUGR by 500 ft due to climbing fast after experiencing some bumps at 10k ft\\', \\'292803\\': \\'LAS - Deviated from the 11k ft restriction at KRUGR by 500 ft due to climbing fast after experiencing some bumps at 10k ft\\', \\'292807\\': \\'BWI - ATC directed go around due to insufficient separation with preceding SWA A/C\\', \\'292808\\': \\'BNA - Concern w/ service provide by AA after not getting the A/C towed to gate until 0625 for a 0700 departure\\', \\'292809\\': \\'BNA - Troubles reaching station ops for gate assignment, Dispatch was able to reach them eventually while on descent\\', \\'292811\\': \\'Enroute - Suspected a possible oil leak after starting from CMH with 14-15 qts of oil and then noticing down to 8 qts over MT\\', \\'292812\\': \\'Enroute - Accepted a clearance to climb 2k ft in 2 minutes or less, Unable to meet clearance after encountering wave activity\\', \\'292814\\': \\'Final compliance walk through was not completed due to turbulence.\\', \\'292816\\': \\'Insufficient time for service.\\', \\'292820\\': \\'Level 2 threat.  Pax disruptive, drinking own alcohol and touching guests and FA and lit a cigarette on AC (Not in lav).   Law enforcement requested\\', \\'292821\\': \\'Level 2 threat.  Pax disruptive, drinking own alcohol and touching guests and FA and lit a cigarette on AC (Not in lav).   Law enforcement requested\\'}, \\'Description_x\\': {\\'292802\\': \\'- Report # 406379\\\\n \\\\nWe were cleared to Climb Via the JOHKR3 Departure, the climb was pretty bumpy and at 10000ft CA decided to stay at 250kts a little longer to clear the clouds hopping for a better ride. I did my 10k ft check and proceeded to get my ipad to check for restrictions but we were climbing fast and by the time I realized we should level at 11K ft at KRUGR, we were already passed and received a call from ATC advising us of the restriction and possible pilot deviation, we climbed 500ft above altitude before the waypoint and descent to altitude for waypoint crossing.\\\\n \\\\n - Report # 406392\\\\n \\\\nWe were cleared to Climb Via the JOHKR3 Departure, the climb was\\\\npretty bumpy and at 10000ft CA decided to stay at 250kts a little longer\\\\nto clear the clouds hopping for a better ride. I did my 10k ft check and\\\\nproceeded to get my ipad to check for restrictions but we were climbing\\\\nfast and by the time I realized we should level at 11K ft at KRUGR, we\\\\nwere already passed and received a call from ATC advising us of the restriction and possible pilot deviation. We climbed 500ft above before\\\\nreaching the waypoint and descent to altitude for waypoint crossing.\\', \\'292803\\': \\'- Report # 406379\\\\n \\\\nWe were cleared to Climb Via the JOHKR3 Departure, the climb was pretty bumpy and at 10000ft CA decided to stay at 250kts a little longer to clear the clouds hopping for a better ride. I did my 10k ft check and proceeded to get my ipad to check for restrictions but we were climbing fast and by the time I realized we should level at 11K ft at KRUGR, we were already passed and received a call from ATC advising us of the restriction and possible pilot deviation, we climbed 500ft above altitude before the waypoint and descent to altitude for waypoint crossing.\\\\n \\\\n - Report # 406392\\\\n \\\\nWe were cleared to Climb Via the JOHKR3 Departure, the climb was\\\\npretty bumpy and at 10000ft CA decided to stay at 250kts a little longer\\\\nto clear the clouds hopping for a better ride. I did my 10k ft check and\\\\nproceeded to get my ipad to check for restrictions but we were climbing\\\\nfast and by the time I realized we should level at 11K ft at KRUGR, we\\\\nwere already passed and received a call from ATC advising us of the restriction and possible pilot deviation. We climbed 500ft above before\\\\nreaching the waypoint and descent to altitude for waypoint crossing.\\', \\'292807\\': \\'- Report # 406530\\\\n \\\\nWeather at BWI was VFR. While on vectors to 33L for a visual approach, we were given a speed of 170 knots and a heading to place us on a right base. We could see a SWA 737 on final to 33L and were visual to the runway, so I as PM confirmed with the PF he had both and we reported it to ATC. He turned us onto a 45 degree intercept and switched us to tower, approximately 10 miles out. On Checkin with tower he cleared us to land but advised us we had 50 knots of overtake on the SWA 737 on short final. He then advised the SWA to expect minimal time on the runway. We were slowing and configuring, and at approximately 1-2 miles tower instructed us to go around due to insufficient separation. We flew a left 360 box pattern and landed uneventfully.\\', \\'292808\\': \\'- Report # 406360\\\\n \\\\nWe arrived at the gate early at 0600, and there was no plane at the gate. American Airlines finally towed the plane to our gate and we boarded at 0625 for an 0700 departure. From the jetway and cabin we could tell if there was frost or just condensation on the wings, so I asked for a ladder so I could do a tactile wing inspection. I was instructed there was no working ladder to use, so we had to deice to be sure the aircraft was free  from frost. The outside temperature was 34 degrees.\\', \\'292809\\': \\'- Report # 406361\\\\n \\\\nWe sent an IN RANGE report on our way to KBNA and received HG as a response for our gate. We asked the flight attendants what their device said and it was depicted as \"unknown.\" About 30 minutes out from KBNA we called on the OPS frequency for the gate assignment, but no one answered. We then sent an ACARS request for dispatch\\\\\\'s help with a gate. They responded that they were calling KBNA OPS and no one would answer. We finally received an ACARS from dispatch descending through 12,000\\\\\\' that our gate was C8. While deplaning I asked the pilots taking our aircraft if they experienced a similar story when they landed the previous day and they said \"YES.\"\\', \\'292811\\': \\'- Report # 406356\\\\n \\\\nFLT 992 CMH - SEA  Noticed a possible oil leak on the #2 engine.  We started from CMH with approx. 14-15 Qts of oil. Over Montana we were down to 8. My co pilot Mike Anderson and I discussed the problem with a review of the QRH and the systems handbook. We talked about a diversion, where it would be and at what point would we would make that decision. We brought Dispatch into the loop and asked them to notify MTX control and the FODO to keep them advised. We were losing a quart of oil about every 40 min. Our diversion options would be BIL, then GEG, then YKM then on to SEA if we thought that was an option. The lowest QTY was 5 quarts halfway between GEG and SEA.  We continued, landed in SEA and upon shutdown at the gate, the Qty came up to 8 on the indicator. In talking with MTX a day later about the plane, it was taken to the hanger and an oil leak was found. We had no abnormal indications on the oil temp or oil press gage at any time. This was aircraft N272AK.  \\\\nWe did not feel it was necessary to discuss the problem with the Flight attendants unless we actually diverted. This was not an engine shutdown but was headed that way. I did not see a better area to make the report.\\', \\'292812\\': \"- Report # 406366\\\\n \\\\nFlight 1001 from SFO to DCA around 11:30am close to the Rockies I believe.  I was issued and accepted a clearance to climb 2000ft in 2 minutes or less.  As we initiated the climb all looked good but we encountered some wave activity that slowed our climb rate.   I tried to trade airspeed for altitude but just couldn\\'t do it within a safe margin.  I tried to call and inform ATC I WAS unable to meet the restriction of 2000ft in 2 minutes and suggest if a heading change would help, No solutions were given.  The traffic in question I could see  and believed to be a safe margin of distance.  We never received an RA or TA.  We continued the flight to the destination without further incident.\", \\'292814\\': \\'- Report # 406227\\\\n \\\\nRemained seated due to turbulence between DLG AKN for final walk through compliance. Made safety announcement and then it got to turbulent to safely walk through cabin checking for compliance.\\', \\'292816\\': \\'- Report # 406265\\\\n \\\\nWe modified service as the flight time is too short to ANC-BET\\', \\'292820\\': \\'- Report # 406323\\\\n \\\\nPax is 18D Tine Watts was on the phone multiple times during the demo. I had to stop and ask her to put her phone in airplane mode. She then didn\\\\\\'t want to put her seatbelt on or stow her bags under the seat. The other flight attendant and I got her to comply and we returned to our jumpseats. During takeoff I heard her arguing with the passengers behind her and at the 10,000 chime I went to ask if there was a problem. I offered the passengers beside her and behind her to move seats and they all said it was okay. The passengers also saw her drink and shot of vodka out of her bag. I notified the pilots. I then saw the bottle she drank from and took a photo of it. I decided that I would need to be in the back to monitor her as she was acting very strange so I changed the beverage service to water. She then is stumbling down the isle and leaning on everyone until we tell her where her seat is. She continues to get up and touch other passengers. I then come up to her and tell her that is not okay behavior and we need her to sit in her seat. She then put her fists up at me as if she were trying to fight. I again ask her to return to her seat. She does and she asks me to come over. I go see if there is anything she needs and she starts rubbing my thigh. I ask her please don\\\\\\'t touch me. She then says \"Fuck you, yeah fuck you\" and I said excuse me. She then gets up and gets in my face and says \"you want a piece of this\" as if she is going to hit me. I told her that we needed her in her seat and that drinking her own alcohol is against federal law and she cannot do that. She then starts to act upset and we are at the ten thousand chime on the way down. We clean up the cabin and secure it. Shortly after turning off the overhead lights I see her light a cigarette. All the passengers around her are also seeing this happen. I call the pilots with and emergency call saying what she just did. I then go up to her and try to get the cigarette to make sure there is no fire onboard. She is saying she didn\\\\\\'t do it and denying everything. The cabin smells of her cigarette and the pilots say we are landing. When we land we have the passengers remain seated so she can be escorted off the airplane\\\\n \\\\n - Report # 406325\\\\n \\\\nPassenger: Tine Watts \\\\nSeat: 18D\\\\nFlight: 2004\\\\nSEA-FCA\\\\n\\\\nPassenger appeared to be intoxicated as we were about to take off. Refused to turn her phone off multiple times. Was arguing and taking pictures of pax behind her. Other pax saw her take a mini bottle of booze out of her purse and drink it inflight. FA B Johanna Randall took a picture of one of the bottles that she threw onto the seat next to her. Pax continued to harass other passengers throughout flight. Main cabin service had to be canceled so she could be watched by FA B. Right before landing Tine lit a cigarette. Johanna witnessed it. She hid it and tried to say she didn\\\\\\'t light it. I assisted Johanna and I could definitely smell it. As well as the captain and most of the cabin. Johanna stated that Tine threatened to fight her twice and said profanities to her throughout the flight. The flight was only 52 minutes with a tail wind so thankfully there wasn\\\\\\'t a lot of time for things to progress much worse. Also, other passengers mentioned she appeared intoxicated at the boarding gate.\\', \\'292821\\': \\'- Report # 406323\\\\n \\\\nPax is 18D Tine Watts was on the phone multiple times during the demo. I had to stop and ask her to put her phone in airplane mode. She then didn\\\\\\'t want to put her seatbelt on or stow her bags under the seat. The other flight attendant and I got her to comply and we returned to our jumpseats. During takeoff I heard her arguing with the passengers behind her and at the 10,000 chime I went to ask if there was a problem. I offered the passengers beside her and behind her to move seats and they all said it was okay. The passengers also saw her drink and shot of vodka out of her bag. I notified the pilots. I then saw the bottle she drank from and took a photo of it. I decided that I would need to be in the back to monitor her as she was acting very strange so I changed the beverage service to water. She then is stumbling down the isle and leaning on everyone until we tell her where her seat is. She continues to get up and touch other passengers. I then come up to her and tell her that is not okay behavior and we need her to sit in her seat. She then put her fists up at me as if she were trying to fight. I again ask her to return to her seat. She does and she asks me to come over. I go see if there is anything she needs and she starts rubbing my thigh. I ask her please don\\\\\\'t touch me. She then says \"Fuck you, yeah fuck you\" and I said excuse me. She then gets up and gets in my face and says \"you want a piece of this\" as if she is going to hit me. I told her that we needed her in her seat and that drinking her own alcohol is against federal law and she cannot do that. She then starts to act upset and we are at the ten thousand chime on the way down. We clean up the cabin and secure it. Shortly after turning off the overhead lights I see her light a cigarette. All the passengers around her are also seeing this happen. I call the pilots with and emergency call saying what she just did. I then go up to her and try to get the cigarette to make sure there is no fire onboard. She is saying she didn\\\\\\'t do it and denying everything. The cabin smells of her cigarette and the pilots say we are landing. When we land we have the passengers remain seated so she can be escorted off the airplane\\\\n \\\\n - Report # 406325\\\\n \\\\nPassenger: Tine Watts \\\\nSeat: 18D\\\\nFlight: 2004\\\\nSEA-FCA\\\\n\\\\nPassenger appeared to be intoxicated as we were about to take off. Refused to turn her phone off multiple times. Was arguing and taking pictures of pax behind her. Other pax saw her take a mini bottle of booze out of her purse and drink it inflight. FA B Johanna Randall took a picture of one of the bottles that she threw onto the seat next to her. Pax continued to harass other passengers throughout flight. Main cabin service had to be canceled so she could be watched by FA B. Right before landing Tine lit a cigarette. Johanna witnessed it. She hid it and tried to say she didn\\\\\\'t light it. I assisted Johanna and I could definitely smell it. As well as the captain and most of the cabin. Johanna stated that Tine threatened to fight her twice and said profanities to her throughout the flight. The flight was only 52 minutes with a tail wind so thankfully there wasn\\\\\\'t a lot of time for things to progress much worse. Also, other passengers mentioned she appeared intoxicated at the boarding gate.\\'}, \\'Recommendations\\': {\\'292802\\': \"The reality is I failed as PM today, I had a better, faster and easier way to\\\\ncheck for the restrictions right in front on MFD and instead I reached out\\\\nto my ipad, combined with the fact we didn\\'t level off to accelerate and\\\\nwere climbing fast, all took part for it to happen but the raw true is that I\\\\nfailed my responsibility today. definitely was a learning experience and\\\\nthought me the importance of my part as PM in a flight and how we can become complacent even though not intentionally.\", \\'292803\\': \"The reality is I failed as PM today, I had a better, faster and easier way to check for the restrictions right in front on MFD and instead I reached out to my ipad, combined with the fact we didn\\'t level off to accelerate and were climbing fast, all took part for it to happen but the raw true is that I failed my responsibility today. definitely was a learning experience and thought me the importance of my part as PM in a flight and how we can become complacent even though not intentionally.\\\\r\\\\n##MR\", \\'292807\\': \\'Utilizing the TCAs and my own SA from the radios, I knew they were trying to fit us in between two separate SWA aircraft already on final. I should have  directed the PF to start slowing to final speed as soon as we turned further inbound and let Approach manage the aircraft behind us.\\\\r\\\\n##MR\\', \\'292808\\': \"I am questioning the service provided by American for Alaska in Nashville. I experienced no one to answer the OPS radio the day prior, a late aircraft arrival at the gate this morning for a STAR flight, and no ladder to give ourselves a proper ice check. I certainly hope this isn\\'t a glimpse into the future for Alaska.\", \\'292809\\': \"Being a pilot with Alaska for 17 years now, I never experienced such a delay in relaying the gate information to us. We have been flying to KBNA for quite a while now, so is this a problem because American Airlines is handling the station? I don\\'t know, but it certainly doesn\\'t help the pilots make a plan for taxiing and deplaning. Were we going to use stairs to deplane since the gate code was HG? We need to be able to tell the flight attendants and passengers the plan. 12,000\\' is way too late to be receiving this information.\", \\'292811\\': \\'The procedures in the Max have us declutter the engine pack and not show all the indications. I recommend we keep all of those in view. If we had a Max on that flight we would not have seen the problem early and would not have been able to discuss a plan of action until the last moment when we had to shut an engine down. It was better to see the problem early and discuss it.\\', \\'292812\\': \\'Just to not accept a clearance if performance is tight and could be compromised due to weather phenomenon.\\', \\'292814\\': \\'Safety is most important!\\', \\'292816\\': \\'Just water like the other shorter flights\\', \\'292820\\': \\'She gets put on a no fly list. And everyone does their job in this situation because her behavior was unacceptable\\', \\'292821\\': \\'Because of the circumstances I strongly feel that Tina Watts should be banned from flying Alaska Airlines. I am hoping she was arrested. The supervisor said he was going to handle it in FCA. When the crew were leaving the airport she was talking to the supervisor and security, crying and telling them a sob story. Tried to say she was served the alcohol on flight and denied lighting a cigarette. The supervisor commented that she seemed very upset and seemed like he was leaning towards letting her go and just writing a report. We emphasized that she broke federal law and we felt more then that needed to happen.\\'}}', 'id': '54', 'metadata': \"['ID', 'Division_x', 'EventDate_x', 'Subject', 'Description_x', 'Recommendations']\", '@search.score': 6.631425, '@search.reranker_score': None, '@search.highlights': None, '@search.captions': None}\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the search results\n",
    "for page in results.by_page():\n",
    "    for result in page:\n",
    "        # Access individual documents\n",
    "        print(result)\n",
    "        # To access the first result, break out of the loop\n",
    "        break\n",
    "    # To access only the first result, break out of the outer loop\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "6e7eebc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['content', 'id', 'metadata', '@search.score', '@search.reranker_score', '@search.highlights', '@search.captions'])\n"
     ]
    }
   ],
   "source": [
    "# Iterate through the search results\n",
    "for page in results.by_page():\n",
    "    for result in page:\n",
    "        # Access individual documents\n",
    "        print(result.keys())\n",
    "        # To access the first result, break out of the loop\n",
    "        break\n",
    "    # To access only the first result, break out of the outer loop\n",
    "    break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "22d3deaa",
   "metadata": {},
   "outputs": [
    {
     "ename": "HttpResponseError",
     "evalue": "() Unknown field 'content_vector' in vector field list.\r\nParameter name: vectorFields\nCode: \nMessage: Unknown field 'content_vector' in vector field list.\r\nParameter name: vectorFields",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mHttpResponseError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[28], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m vector \u001b[38;5;241m=\u001b[39m Vector(value\u001b[38;5;241m=\u001b[39m query_embeddings, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m3\u001b[39m, fields\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent_vector\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     14\u001b[0m results \u001b[38;5;241m=\u001b[39m search_client_v2\u001b[38;5;241m.\u001b[39msearch(  \n\u001b[1;32m     15\u001b[0m     search_text\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,  \n\u001b[1;32m     16\u001b[0m     vectors\u001b[38;5;241m=\u001b[39m [vector],\n\u001b[1;32m     17\u001b[0m     select\u001b[38;5;241m=\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m\"\u001b[39m ,\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m\"\u001b[39m],\n\u001b[1;32m     18\u001b[0m )  \n\u001b[0;32m---> 20\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result \u001b[38;5;129;01min\u001b[39;00m results:  \n\u001b[1;32m     21\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mid: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mid\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n\u001b[1;32m     22\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mScore: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mresult[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m@search.score\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)  \n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/azure/search/documents/_paging.py:54\u001b[0m, in \u001b[0;36mSearchItemPaged.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     52\u001b[0m     first_iterator \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_first_iterator_instance()\n\u001b[1;32m     53\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator \u001b[38;5;241m=\u001b[39m itertools\u001b[38;5;241m.\u001b[39mchain\u001b[38;5;241m.\u001b[39mfrom_iterable(first_iterator)\n\u001b[0;32m---> 54\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_page_iterator)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/azure/core/paging.py:75\u001b[0m, in \u001b[0;36mPageIterator.__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mStopIteration\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mEnd of paging\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     74\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 75\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_response \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_next(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontinuation_token)\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m AzureError \u001b[38;5;28;01mas\u001b[39;00m error:\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m error\u001b[38;5;241m.\u001b[39mcontinuation_token:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/azure/search/documents/_paging.py:124\u001b[0m, in \u001b[0;36mSearchPageIterator._get_next_cb\u001b[0;34m(self, continuation_token)\u001b[0m\n\u001b[1;32m    122\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_get_next_cb\u001b[39m(\u001b[38;5;28mself\u001b[39m, continuation_token):\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m continuation_token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 124\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39msearch_post(search_request\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_initial_query\u001b[38;5;241m.\u001b[39mrequest, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n\u001b[1;32m    126\u001b[0m     _next_link, next_page_request \u001b[38;5;241m=\u001b[39m unpack_continuation_token(continuation_token)\n\u001b[1;32m    128\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_client\u001b[38;5;241m.\u001b[39mdocuments\u001b[38;5;241m.\u001b[39msearch_post(search_request\u001b[38;5;241m=\u001b[39mnext_page_request, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_kwargs)\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/azure/core/tracing/decorator.py:78\u001b[0m, in \u001b[0;36mdistributed_trace.<locals>.decorator.<locals>.wrapper_use_tracer\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m span_impl_type \u001b[38;5;241m=\u001b[39m settings\u001b[38;5;241m.\u001b[39mtracing_implementation()\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m span_impl_type \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m---> 78\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m func(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     80\u001b[0m \u001b[38;5;66;03m# Merge span is parameter is set, but only if no explicit parent are passed\u001b[39;00m\n\u001b[1;32m     81\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m merge_span \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m passed_in_parent:\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.11/site-packages/azure/search/documents/_generated/operations/_documents_operations.py:787\u001b[0m, in \u001b[0;36mDocumentsOperations.search_post\u001b[0;34m(self, search_request, request_options, **kwargs)\u001b[0m\n\u001b[1;32m    785\u001b[0m     map_error(status_code\u001b[38;5;241m=\u001b[39mresponse\u001b[38;5;241m.\u001b[39mstatus_code, response\u001b[38;5;241m=\u001b[39mresponse, error_map\u001b[38;5;241m=\u001b[39merror_map)\n\u001b[1;32m    786\u001b[0m     error \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize\u001b[38;5;241m.\u001b[39mfailsafe_deserialize(_models\u001b[38;5;241m.\u001b[39mSearchError, pipeline_response)\n\u001b[0;32m--> 787\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m HttpResponseError(response\u001b[38;5;241m=\u001b[39mresponse, model\u001b[38;5;241m=\u001b[39merror)\n\u001b[1;32m    789\u001b[0m deserialized \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_deserialize(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSearchDocumentsResult\u001b[39m\u001b[38;5;124m\"\u001b[39m, pipeline_response)\n\u001b[1;32m    791\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mcls\u001b[39m:\n",
      "\u001b[0;31mHttpResponseError\u001b[0m: () Unknown field 'content_vector' in vector field list.\r\nParameter name: vectorFields\nCode: \nMessage: Unknown field 'content_vector' in vector field list.\r\nParameter name: vectorFields"
     ]
    }
   ],
   "source": [
    "# Pure Vector Search\n",
    "query = \"Are we also seeing similar trends with Towbars?\"\n",
    "\n",
    "embeddings = OpenAIEmbeddings(deployment = model,chunk_size=1) \n",
    "\n",
    "query_embeddings = embeddings.embed_query(query)\n",
    "\n",
    "# Convert the query embeddings to a list\n",
    "#query_embeddings_list = query_embeddings.tolist()\n",
    " \n",
    "#search_client = SearchClient(service_endpoint, index_name, credential=credential)\n",
    "vector = Vector(value= query_embeddings, k=3, fields=\"content_vector\")\n",
    "  \n",
    "results = search_client_v2.search(  \n",
    "    search_text=None,  \n",
    "    vectors= [vector],\n",
    "    select=[\"id\" ,\"content\", \"metadata\"],\n",
    ")  \n",
    "  \n",
    "for result in results:  \n",
    "    print(f\"id: {result['id']}\")  \n",
    "    print(f\"Score: {result['@search.score']}\")  \n",
    "    print(f\"content: {result['content']}\") \n",
    "    print(f\"page_number: {result['metadata']}\\n\") \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52310fb4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
