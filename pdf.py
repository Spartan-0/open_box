{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19fdf471",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import required libraries  \n",
    "import os  \n",
    "import json  \n",
    "import openai  \n",
    "from tenacity import retry, wait_random_exponential, stop_after_attempt  \n",
    "from azure.core.credentials import AzureKeyCredential  \n",
    "from azure.search.documents import SearchClient  \n",
    "from azure.search.documents.indexes import SearchIndexClient  \n",
    "from azure.search.documents.models import Vector  \n",
    "from azure.search.documents.indexes.models import (  \n",
    "    SearchIndex,  \n",
    "    SearchField,  \n",
    "    SearchFieldDataType,  \n",
    "    SimpleField,  \n",
    "    SearchableField,  \n",
    "    SearchIndex,  \n",
    "    SemanticConfiguration,  \n",
    "    PrioritizedFields,  \n",
    "    SemanticField,  \n",
    "    SemanticSettings,  \n",
    "    VectorSearch, \n",
    "    HnswVectorSearchAlgorithmConfiguration,  \n",
    ")  \n",
    "\n",
    "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
    "from langchain.vectorstores.azuresearch import AzureSearch\n",
    "from langchain.document_loaders import AzureBlobStorageContainerLoader\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from azure.storage.blob import BlobServiceClient\n",
    "import tempfile\n",
    "from langchain.document_loaders import PyPDFLoader\n",
    "from azure.core.exceptions import HttpResponseError \n",
    "from langchain.embeddings.openai import OpenAIEmbeddings\n",
    "# Configure environment variables  \n",
    "load_dotenv()  \n",
    "service_endpoint = os.getenv(\"AZURE_SEARCH_SERVICE_ENDPOINT\") \n",
    "index_name = os.getenv(\"AZURE_SEARCH_INDEX_NAME\") \n",
    "key = os.getenv(\"AZURE_SEARCH_ADMIN_KEY\") \n",
    "openai.api_type = \"azure\"  \n",
    "openai.api_key = os.getenv(\"AZURE_OPENAI_API_KEY\")  \n",
    "openai.api_base = os.getenv(\"AZURE_OPENAI_ENDPOINT\")  \n",
    "openai.api_version = os.getenv(\"AZURE_OPENAI_API_VERSION\")  \n",
    "credential = AzureKeyCredential(\"jA50VnyT0OAeuHpY0RxJGz7z9kfrmdj7YF5PHYiPXjAzSeBS6hJg\")\n",
    "os.environ[\"OPENAI_API_TYPE\"] = \"azure\"\n",
    "os.environ[\"OPENAI_API_BASE\"] = \"https://testingchat.openai.azure.com/\"\n",
    "os.environ[\"OPENAI_API_KEY\"] = \"e8143eabb02541259054426630db12a7\"\n",
    "os.environ[\"OPENAI_API_VERSION\"] = \"2023-05-15\"\n",
    "model: str = \"text-embedding-ada-002\"\n",
    "#pdf_file_name = \"Alaska Airlines and Horizon Air SMS Manual.pdf\"     \n",
    "\n",
    "AZURE_SEARCH_SERVICE_ENDPOINT = \"https://genaisafety.search.windows.net\"\n",
    "AZURE_SEARCH_ADMIN_KEY = \"jA50VnyT0OAeuHpY0RxJGz7z9kfrmdj7YF5PHYiPXjAzSeBS6hJg\"\n",
    "def pdf_text_extraction(pdf_file_path):\n",
    "    if pdf_file_path is not None:\n",
    "        with open(pdf_file_path, 'rb') as pdf_file:\n",
    "            # Create a temporary file to write the PDF contents\n",
    "            with tempfile.NamedTemporaryFile(delete=False) as tmp_file:\n",
    "                tmp_file.write(pdf_file.read())\n",
    "                pdf_path = tmp_file.name\n",
    "\n",
    "        # Now you can use the PyPDFLoader on the temporary PDF file\n",
    "        loader = PyPDFLoader(pdf_path)\n",
    "        pages_split = loader.load_and_split()\n",
    "    return pages_split\n",
    "def create_index(AZURE_SEARCH_INDEX_NAME, fields):\n",
    "    vector_search = VectorSearch(\n",
    "        algorithm_configurations=[\n",
    "            HnswVectorSearchAlgorithmConfiguration(\n",
    "                name=\"my-vector-config\",\n",
    "                kind=\"hnsw\",\n",
    "                parameters={\n",
    "                    \"m\": 4,\n",
    "                    \"efConstruction\": 400,\n",
    "                    \"efSearch\": 500,\n",
    "                    \"metric\": \"cosine\"\n",
    "                }\n",
    "            )\n",
    "        ]\n",
    "    )\n",
    "\n",
    "\n",
    "    semantic_config = SemanticConfiguration(\n",
    "        name=\"my-semantic-config\",\n",
    "        prioritized_fields=PrioritizedFields(\n",
    "            title_field=SemanticField(field_name=\"id\"),\n",
    "            prioritized_keywords_fields=[SemanticField(field_name=\"content\")]\n",
    "        )\n",
    "    )\n",
    "\n",
    "    # Create the semantic settings with the configuration\n",
    "    semantic_settings = SemanticSettings(configurations=[semantic_config])\n",
    "\n",
    "    # Create the search index with the semantic settings\n",
    "    index = SearchIndex(name=AZURE_SEARCH_INDEX_NAME, fields=fields,\n",
    "                        vector_search=vector_search, semantic_settings=semantic_settings)\n",
    "\n",
    "    result = search_client.create_or_update_index(index)\n",
    "    return result\n",
    "embeddings = OpenAIEmbeddings(deployment = model,chunk_size=1) \n",
    "\n",
    "# Replace with your actual connection string\n",
    "connection_string = \"DefaultEndpointsProtocol=https;AccountName=storageaccountgenai;AccountKey=WAfyL4uqWgarFdx1ibNgWv9lmOINODLuN6nnLSQLgE/iuHhKGi1pYd6NQJ6LBnZO/DnnQfhbNSWi+AStsOLf1Q==;EndpointSuffix=core.windows.net\"\n",
    "\n",
    "# Replace with your container name\n",
    "container_name = \"safety\"\n",
    "\n",
    "# Create a BlobServiceClient using the connection string\n",
    "blob_service_client = BlobServiceClient.from_connection_string(connection_string)\n",
    "\n",
    "# Get a reference to the container\n",
    "container_client = blob_service_client.get_container_client(container_name)\n",
    "# List blobs in the container\n",
    "blobs = container_client.list_blobs()\n",
    "dict_data = []\n",
    "for blob in blobs:\n",
    "    if blob.name.startswith(\"manuals\") and blob.name.endswith(\".pdf\"): #and (\"manuals/Alaska Airlines and Horizon Air SMS Manual.pdf\" in blob.name):  # Check if the blob is a PDF file\n",
    "        #blob_client = container_client.get_blob_client(blob)\n",
    "        pdf_file_name=str(blob.name).split(\"/\")[1] \n",
    "        bid=str(blob.name).split(\"/\")[1].split(\".\")[0].lower().replace(\" \",\"_\").replace(\"_\",\"-\")\n",
    "        print(blob.name)\n",
    "        \n",
    "                      \n",
    "        documents = pdf_text_extraction(pdf_file_name)\n",
    "\n",
    "        text_splitter = CharacterTextSplitter(chunk_size=1000, chunk_overlap=0)\n",
    "        docs = text_splitter.split_documents(documents)\n",
    "        print(\"PDF_NAME, length\", blob.name, len(docs))\n",
    "        \n",
    "        \n",
    "            \n",
    "        for index,doc in enumerate(docs):\n",
    "            dict_data.append({\n",
    "                'id': bid+'_'+ str(index),\n",
    "                'content': doc.page_content,  # You can set a title based on your needs\n",
    "                'page_number': doc.metadata['page'],\n",
    "                'metadata': doc.metadata,\n",
    "            })\n",
    "\n",
    "        print(f\"splitted into {len(docs)} documents\") \n",
    "    \n",
    "        \n",
    "\n",
    "        for item in dict_data:\n",
    "            title = item['id']\n",
    "            content = item['content']\n",
    "            title_embeddings = embeddings.embed_query(title)\n",
    "            content_embeddings = embeddings.embed_query(content)\n",
    "            item['titleVector'] = title_embeddings\n",
    "            item['content_vector'] = content_embeddings\n",
    "            \n",
    "        bid=str(blob.name).split(\"/\")[1].split(\".\")[0].lower().replace(\" \",\"_\").replace(\"_\",\"-\")\n",
    "        AZURE_SEARCH_INDEX_NAME=bid + \"-index\"\n",
    "        print(AZURE_SEARCH_INDEX_NAME)\n",
    "\n",
    "        \n",
    "        search_client = SearchIndexClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, credential=credential)\n",
    "        search_client_v2 = SearchClient(endpoint=AZURE_SEARCH_SERVICE_ENDPOINT, index_name=AZURE_SEARCH_INDEX_NAME, credential=credential)\n",
    "\n",
    "        \n",
    "        # Define your Azure Cognitive Search credentials and index settings as previously done\n",
    "        # Check if the index already exists\n",
    "        # existing_index = None\n",
    "        # Define your Azure Cognitive Search credentials and index settings as previously done\n",
    "\n",
    "        combined_content = ''.join([p.page_content for p in docs])\n",
    "        combined_vector = embeddings.embed_query(combined_content)\n",
    "        try:\n",
    "            existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "        except Exception as e:\n",
    "            existing_index == None\n",
    "            # The index doesn't exist, so create a new one\n",
    "        if existing_index == None:\n",
    "            fields = [\n",
    "            SimpleField(name=\"id\", type=SearchFieldDataType.String, key=True, sortable=True, filterable=True, facetable=True),\n",
    "            SearchableField(name=\"title\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"content\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"combined_content\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"page_number\", type=SearchFieldDataType.String),\n",
    "            SearchableField(name=\"metadata\", type=SearchFieldDataType.String, searchable=True),\n",
    "            SearchField(name=\"combined_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                        searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"my-vector-config\"),\n",
    "            SearchField(name=\"content_vector\", type=SearchFieldDataType.Collection(SearchFieldDataType.Single),\n",
    "                        searchable=True, vector_search_dimensions=1536, vector_search_configuration=\"my-vector-config\"),\n",
    "            ]\n",
    "\n",
    "            result = create_index(AZURE_SEARCH_INDEX_NAME, fields)\n",
    "            print(f' {result.name} created index')                        \n",
    "            \n",
    "            final_doc=[]\n",
    "            for index,doc in enumerate(docs):\n",
    "                final_doc.append({'id':dict_data[index]['id'], 'title':dict_data[index]['id'],\n",
    "                                  'content':dict_data[index]['content'],\n",
    "                                  'combined_content':combined_content,\n",
    "                                  'page_number':str(dict_data[index]['page_number']), \n",
    "                                  'metadata': json.dumps(dict_data[index]['metadata']),\n",
    "                                  'combined_vector': combined_vector, 'content_vector':dict_data[index]['content_vector']})\n",
    "                \n",
    "            # Define a list to store batches of documents\n",
    "            document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "            # Upload documents in batches\n",
    "            for batch in document_batches:\n",
    "                try:\n",
    "                    result = search_client_v2.upload_documents(batch)\n",
    "                except HttpResponseError as e:\n",
    "                    print(f\"Error uploading documents: {e}\")    \n",
    "\n",
    "            print(\"Vector store added successfully\")\n",
    "\n",
    "        else:\n",
    "            try:\n",
    "                # Check if the index exists\n",
    "                # existing_index = search_client.get_index(AZURE_SEARCH_INDEX_NAME)\n",
    "                # print(f\"Index '{AZURE_SEARCH_INDEX_NAME}' already exists. You can use the existing index.\")\n",
    "\n",
    "                final_doc=[]\n",
    "                metadata_str = json.dumps(dict_data[index]['metadata'])\n",
    "\n",
    "                for index,doc in enumerate(docs):\n",
    "                    final_doc.append({'id':dict_data[index]['id'], 'title':dict_data[index]['id'],\n",
    "                                      'content':dict_data[index]['content'],\n",
    "                                      'combined_content':combined_content,\n",
    "                                      'page_number':str(dict_data[index]['page_number']), \n",
    "                                      'metadata': metadata_str,\n",
    "                                      'combined_vector':combined_vector, 'content_vector':dict_data[index]['content_vector']})\n",
    "\n",
    "                # Define a list to store batches of documents\n",
    "                document_batches = [final_doc[i] for i in range(0, len(final_doc))]\n",
    "\n",
    "                # Upload documents in batches\n",
    "                for batch in document_batches:\n",
    "                    try:\n",
    "                        result = search_client_v2.upload_documents(batch)\n",
    "                    except HttpResponseError as e:\n",
    "                        print(f\"Error uploading documents: {e}\")    \n",
    "\n",
    "                print(\"Vector store added successfully\")\n",
    "            \n",
    "\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "                pass\n",
    "    else:\n",
    "        continue"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
